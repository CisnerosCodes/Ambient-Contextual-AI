# Ambient Contextual AI

> **Zero-Shot Latent Space Analysis for Real-Time Productivity Intelligence**

[![Python](https://img.shields.io/badge/Python-3.10+-blue.svg)](https://python.org)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)
[![CLIP](https://img.shields.io/badge/Model-CLIP--ViT--B--32-orange.svg)](https://openai.com/research/clip)
[![Ollama](https://img.shields.io/badge/LLM-Ollama-purple.svg)](https://ollama.com)

<p align="center">
  <img src="assets/dashboard_preview.png" alt="Dashboard Preview" width="700">
</p>

## ðŸ“‹ Executive Summary

This project implements an intelligent, automated system for digitizing and analyzing user workflow context in real-time. By leveraging state-of-the-art **Computer Vision (CV)** and **Natural Language Processing (NLP)** techniques, the system transforms raw visual data into high-dimensional vector embeddings.

This allows for **"Zero-Shot" classification and analysis**â€”meaning the system can quantify focus and generate semantic narratives **without requiring task-specific model training**.

### âœ¨ Key Features

| Feature | Description |
|---------|-------------|
| ðŸŽ¯ **Focus Quantification** | Measures productivity using cosine similarity in 512-dim latent space |
| ðŸ“– **Automated Narratives** | Local LLM generates hourly activity summaries |
| ðŸ” **Semantic Search** | Query your visual history with natural language |
| ðŸ”’ **100% Offline** | All processing runs locallyâ€”zero cloud dependencies |

---

## ðŸ—ï¸ Technical Architecture

The system operates on a modular architecture designed for local execution, ensuring data privacy and low latency.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    AMBIENT CONTEXTUAL AI                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚   SENSOR     â”‚â”€â”€â”€â–¶â”‚   ANALYSIS   â”‚â”€â”€â”€â–¶â”‚  DASHBOARD   â”‚      â”‚
â”‚  â”‚  (sensor.py) â”‚    â”‚ (analysis.py)â”‚    â”‚(dashboard.py)â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚         â”‚                   â”‚                   â”‚               â”‚
â”‚         â–¼                   â–¼                   â–¼               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚  CLIP ViT    â”‚    â”‚   Ollama     â”‚    â”‚  Streamlit   â”‚      â”‚
â”‚  â”‚  Embeddings  â”‚    â”‚   LLM        â”‚    â”‚  + Plotly    â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                              â”‚                                  â”‚
â”‚                              â–¼                                  â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚
â”‚                    â”‚   SQLite + JSON  â”‚                        â”‚
â”‚                    â”‚  (Vector Store)  â”‚                        â”‚
â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1. ðŸ‘ï¸ Visual Semantic Embedding Engine (Computer Vision)

| Component | Details |
|-----------|--------|
| **Technology** | `sentence-transformers` (CLIP: Contrastive Language-Image Pre-training) |
| **Vector Dimension** | 512-dimensional latent space |
| **Similarity Metric** | Cosine Similarity for focus quantification (0.0 â†’ 1.0) |

The system captures visual data and projects it into a high-dimensional vector space, enabling mathematical comparison between states.

### 2. ðŸ¤– Automated Narrative Generation (Generative AI)

| Component | Details |
|-----------|--------|
| **Technology** | `Ollama` (Llama 3 / Gemma - Local Inference) |
| **Input** | OCR-extracted text from screenshots |
| **Output** | Human-readable hourly activity summaries |

Transforms raw sensor data into semantic context: *"What was the user working on?"*

### 3. ðŸ“Š Real-Time Intelligence Dashboard

| Feature | Description |
|---------|-------------|
| **Focus Wave** | Temporal visualization of cognitive load and focus consistency |
| **Semantic Search** | Natural language querying of visual history |
| **Daily Narrative** | LLM-generated summaries of work sessions |

---

## ðŸ› ï¸ Tech Stack

| Category | Technologies |
|----------|-------------|
| **Language** | Python 3.10+ |
| **Computer Vision** | CLIP-ViT-B-32, Pillow, OpenCV |
| **Generative AI** | Ollama (Llama 3 / Gemma) |
| **OCR** | Tesseract 5.0 |
| **Database** | SQLite + JSON (Vector Storage) |
| **Frontend** | Streamlit, Plotly Express |

---

## ðŸš€ Quick Start

### Prerequisites

- [x] Python 3.10+
- [x] [Ollama](https://ollama.com/) (for local LLM inference)
- [x] [Tesseract OCR](https://github.com/UB-Mannheim/tesseract/wiki) (for text extraction)

### Installation

```bash
# Clone the repository
git clone https://github.com/CisnerosCodes/Ambient-Contextual-AI.git
cd Ambient-Contextual-AI

# Create virtual environment
python -m venv .venv
.venv\Scripts\activate  # Windows
# source .venv/bin/activate  # Linux/Mac

# Install dependencies
pip install -r requirements.txt

# Download LLM model
ollama pull llama3
```

### Usage

```bash
# Terminal 1: Start the sensor (runs in background)
python sensor.py

# Terminal 2: Launch the dashboard
streamlit run dashboard.py
```

> ðŸ’¡ **Tip**: Set your "Anchor" (ideal work state) in the dashboard sidebar to start tracking focus.

---

## ðŸ“ˆ Research & Development Objectives

1. **Zero-Shot Recognition**: Validate CLIP embeddings for unsupervised activity classification
2. **Privacy-First Design**: 100% offline processing with no cloud dependencies
3. **Multimodal Understanding**: Bridge raw pixels â†’ semantic meaning using vision-language models

---

## ðŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

<p align="center">
  <b>Developed by Adrian Cisneros</b><br>
  <i>R&D in Computer Vision and Intelligent Systems</i>
</p>
